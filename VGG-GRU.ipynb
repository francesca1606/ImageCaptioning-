{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning using VGG + GRU  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize, GaussianBlur\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file captions.txt is a txt file where each row presents an image_name, the comment_number and the comment, since each image has 5 different captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_file_path = \"./archive.zip\"\n",
    "extract_folder = \"./archive\"\n",
    "\n",
    "# with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "#    zip_ref.extractall(extract_folder)\n",
    "\n",
    "images_path = os.path.join(extract_folder, \"flickr30k_images\")\n",
    "captions_path = \"./captions.txt\"\n",
    "\n",
    "archive_df = pd.read_csv(captions_path,sep=\",\",header=None,names=[\"image_name\", \"caption_id\", \"caption\"],skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to feed the captions to train the model, we want to create a vocabulary of words. We employ the library nltk to define word tokens. A frequency is assigned to each word, and the word is insterted in the vocabulary if frequency >= frequency_threshold, considering the entire dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\franc\\.conda\\envs\\dml\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\franc\\.conda\\envs\\dml\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\franc\\.conda\\envs\\dml\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\franc\\.conda\\envs\\dml\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\franc\\.conda\\envs\\dml\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\franc\\.conda\\envs\\dml\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\franc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    def one_hot_encode_caption(self, caption):\\n        one_hot_vectors = [self.one_hot_encode(word) for word in caption]\\n        return torch.tensor(one_hot_vectors)\\n    def numericalize(self, sentence):\\n        sentence = self.preprocess(sentence)\\n        tokenized_text = word_tokenize(sentence.lower())\\n        return [\\n            self.word2idx.get(word, self.word2idx[\"<UNK>\"]) for word in tokenized_text\\n        ]\\ndef caption_to_indices(vocab, caption):\\n    indices = [vocab.word2idx[\"<SOS>\"]]\\n    for word in caption.split():\\n        indices.append(vocab.word2idx.get(word.lower(), \"<UNK>\"))\\n    indices.append(vocab.word2idx[\"<EOS>\"])\\n    return indices\\ndef pad_sequence(seq, target_len, pad_idx):\\n    if len(seq) < target_len:\\n        seq.extend(\\n            [pad_idx] * (target_len - len(seq))\\n        )  # padding at the end of sequence\\n    return seq[:target_len]\\n# caption_indices_list = [caption_to_indices(our_vocab, caption) for caption in captions_list]\\n# pad_idx = our_vocab.word2idx[\\'<PAD>\\']\\n# caption_indices_padded = [pad_sequence(indices, target_len, pad_idx) for indices in caption_indices_list]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=5):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.wheights = {}      #calculated as inverse of the frequency\n",
    "        self.idx = 0\n",
    "\n",
    "        self.add_word(\"<PAD>\")  #to have all sentences of same target_len\n",
    "        self.add_word(\"<SOS>\")  #start of sentence\n",
    "        self.add_word(\"<EOS>\")  #end of sentence\n",
    "        self.add_word(\"<UNK>\")  #unknown word\n",
    "        self.counter = None     #to count the frequency of each word\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        \"\"\"\"\n",
    "        Build vocabulary from a list of sentences based on word frequencies.\n",
    "        \n",
    "        Args:\n",
    "            sentence_list (list of str): A list of sentences for vocabulary creation.\n",
    "\n",
    "        The method processes each sentence to:\n",
    "        - preprocess and tokenize it.\n",
    "        - count word frequencies.\n",
    "        - add words that meet or exceed the frequency threshold to the vocabulary,assigning weights as the inverse of their frequencies.\n",
    "\n",
    "        Returns:\n",
    "            None: Modifies the vocabulary and weights in place.\n",
    "        \"\"\"\n",
    "        frequencies = Counter()\n",
    "        for sentence in sentence_list:\n",
    "            sentence = self.preprocess(sentence)\n",
    "            words = word_tokenize(sentence.lower())\n",
    "            frequencies.update(words)\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.add_word(word)\n",
    "                self.wheights[word] = 1 / freq\n",
    "        self.counter = frequencies\n",
    "\n",
    "\n",
    "    #auxiliary methods to build_vocabulary:\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def preprocess(self, sentence):\n",
    "        return \"\".join([char for char in sentence if char.isalnum() or char.isspace()])\n",
    "\n",
    "\n",
    "    def one_hot_encode(self, word_or_idx):\n",
    "        if isinstance(word_or_idx, str):                                         #If it's a word\n",
    "            idx = self.word2idx.get(word_or_idx, self.word2idx[\"<UNK>\"])\n",
    "        elif isinstance(word_or_idx, int):                                       #If it's already an index\n",
    "            idx = word_or_idx\n",
    "        else:\n",
    "            raise ValueError(\"Input must be a word (str) or an index (int).\")\n",
    "        one_hot_vector = torch.zeros(len(self.word2idx), dtype=torch.float32)\n",
    "        one_hot_vector[idx] = 1.0\n",
    "        return one_hot_vector\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build our own vocabulary using all the captions from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the vocabulary is:  5464\n"
     ]
    }
   ],
   "source": [
    "captions_list = archive_df[\"caption\"].tolist()\n",
    "if os.path.isfile(\"our_vocab.pkl\"):\n",
    "    our_vocab = torch.load(\"our_vocab.pkl\")\n",
    "else:\n",
    "    our_vocab = Vocabulary(freq_threshold=10)\n",
    "    our_vocab.build_vocabulary(captions_list)\n",
    "\n",
    "vocab_size = len(our_vocab)\n",
    "print(\"The length of the vocabulary is: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to give different weights to the special tokens, since they are the ones that appear more frequently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without wheights:  set()\n",
      "Print the weights of the first 30 words:\n",
      " tensor([0.0000e+00, 0.0000e+00, 1.8302e-04, 1.8302e-04, 4.6206e-05, 7.5654e-05,\n",
      "        1.5198e-03, 2.7619e-05, 1.9608e-02, 4.5025e-04, 7.5301e-04, 6.1504e-05,\n",
      "        2.4771e-04, 6.4267e-04, 8.5390e-05, 1.5106e-03, 2.9138e-04, 1.1981e-05,\n",
      "        1.5878e-05, 2.5381e-03, 7.5884e-05, 4.0323e-03, 4.9515e-05, 2.1268e-04,\n",
      "        3.3156e-04, 7.7519e-04, 1.0526e-02, 1.0529e-04, 1.9153e-04, 1.0616e-03])\n"
     ]
    }
   ],
   "source": [
    "words_in_vocab = set(our_vocab.word2idx.keys())\n",
    "words_in_wheights = set(our_vocab.wheights.keys())\n",
    "unwheighted_words = words_in_vocab - words_in_wheights\n",
    "print(\"Words without wheights: \", unwheighted_words)\n",
    "\n",
    "our_vocab.wheights[\"<PAD>\"] = 0\n",
    "our_vocab.wheights[\"<SOS>\"] = 0\n",
    "our_vocab.wheights[\"<EOS>\"] = 1 / len(our_vocab)\n",
    "our_vocab.wheights[\"<UNK>\"] = 1 / len(our_vocab)\n",
    "\n",
    "class_weights = torch.tensor(\n",
    "    [our_vocab.wheights[our_vocab.idx2word[i]] for i in range(len(our_vocab))]\n",
    ")\n",
    "\n",
    "print(\"Print the weights of the first 30 words:\\n\", class_weights[:30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the architecture that we are going to define and its training, we have decided to set the maximum length of the output captions. We first determine the average length of the dataset captions and then cut/pad them in case they are longer/shorter. This is also done to avoid having too many padding indices in the index representation of the captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The target length of the generated captions will be  15\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "average_caption_length = sum(len(caption.split()) for caption in captions_list) / len(captions_list)\n",
    "target_len = math.ceil(average_caption_length) + 1   #+1 to add the <EOS> token\n",
    "print(\"The target length of the generated captions will be \", target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two men are standing outside\n",
      "tensor([ 4, 27, 22, 30, 23,  0,  0,  0,  0,  0,  0,  0,  0,  0,  2])\n",
      "tensor([ 4, 27, 22, 30, 23,  4, 27, 22, 30, 23,  4, 27, 22, 30,  2])\n"
     ]
    }
   ],
   "source": [
    "def caption_to_padded_indices(vocab, caption, target_len):\n",
    "    \"\"\"\n",
    "    Converts a caption to a tensor of indices with padding.\n",
    "\n",
    "    Args:\n",
    "        vocab: Vocabulary object with word-to-index mappings (`word2idx`).\n",
    "        caption (str): The caption text to convert to indices.\n",
    "        target_len (int): The desired length of the output tensor.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of indices of length `target_len`, padded with the index for `<PAD>` if needed, or truncated if too long.\n",
    "        \n",
    "    Pads with `<PAD>` tokens if `indices` length is less than `target_len`, and appends `<EOS>` at the end of the caption before padding or truncating.\n",
    "    \"\"\"\n",
    "    target_len -= 1\n",
    "\n",
    "    indices = []  \n",
    "\n",
    "    for word in caption.split():\n",
    "        indices.append(vocab.word2idx.get(word.lower(), vocab.word2idx[\"<UNK>\"]))\n",
    "\n",
    "    pad_idx = vocab.word2idx[\"<PAD>\"]\n",
    "    if len(indices) < target_len:\n",
    "        indices.extend([pad_idx] * (target_len - len(indices)))\n",
    "    else:\n",
    "        indices = indices[:target_len]\n",
    "    indices.append(vocab.word2idx[\"<EOS>\"])\n",
    "    \n",
    "    return torch.tensor(indices)\n",
    "\n",
    "\n",
    "#Example\n",
    "print(\"Two men are standing outside\")\n",
    "print(caption_to_padded_indices(our_vocab, \"two men are standing outside\", target_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Division into training, validation and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in the dataset each image appears 5 times with different captions, we need to make sure that in the trainining, validation and test sets there are different images, otherwise validation and testing wouldn't be done on unseen data. We first split with percentages 70 - 20 - 10; to increase the number of training data, we use all 5 captions for each image, therefore repeating images in the train, but keep unique images in the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:\t 111245\n",
      "Validation dataset size: 6356\n",
      "Test dataset size:\t 3178\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "image_capidx_df = pd.DataFrame(\n",
    "    {\n",
    "        \"image_name\": archive_df[\"image_name\"],\n",
    "        \"caption\": archive_df[\"caption\"],\n",
    "    }\n",
    ")\n",
    "#print(image_capidx_df.head)\n",
    "\n",
    "#For the first splitting, consider only unique images\n",
    "unique_images = image_capidx_df[\"image_name\"].unique()\n",
    "num_images = len(unique_images)\n",
    "unique_images_df = pd.DataFrame(unique_images, columns=[\"image_name\"])\n",
    "\n",
    "np.random.seed(42)  #for reproducibility\n",
    "np.random.shuffle(unique_images)\n",
    "\n",
    "train_images, val_images, test_images = random_split(range(num_images), [0.7, 0.2, 0.1])\n",
    "\n",
    "train_image_names = unique_images_df.iloc[train_images.indices].image_name.tolist()\n",
    "val_image_names = unique_images_df.iloc[val_images.indices].image_name.tolist()\n",
    "test_image_names = unique_images_df.iloc[test_images.indices].image_name.tolist()\n",
    "\n",
    "train_df = image_capidx_df[\n",
    "    image_capidx_df[\"image_name\"].isin(train_image_names)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "val_df = (\n",
    "    image_capidx_df[image_capidx_df[\"image_name\"].isin(val_image_names)]\n",
    "    .groupby(\"image_name\", as_index=False)\n",
    "    .agg({\"caption\": lambda x: x.sample(1).values[0]})  #Randomly select one caption\n",
    ").reset_index(drop=True)\n",
    "\n",
    "test_df = (\n",
    "    image_capidx_df[image_capidx_df[\"image_name\"].isin(test_image_names)]\n",
    "    .groupby(\"image_name\", as_index=False)\n",
    "    .agg({\"caption\": lambda x: x.sample(1).values[0]})  #Randomly select one caption\n",
    ").reset_index(drop=True)\n",
    "\n",
    "print(f\"Train dataset size:\\t {train_df.shape[0]}\")   #contains 5 repetition per image but with different captions\n",
    "print(f\"Validation dataset size: {val_df.shape[0]}\")  #only contains unique images\n",
    "print(f\"Test dataset size:\\t {test_df.shape[0]}\")     #only contains unique images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class Flickr30kImages(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.dataframe.iloc[idx][\"image_name\"]\n",
    "        caption = self.dataframe.iloc[idx][\"caption\"]\n",
    "        tensor_captioning_indices = caption_to_padded_indices(\n",
    "            our_vocab, caption, target_len\n",
    "        )\n",
    "\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, tensor_captioning_indices\n",
    "\n",
    "    def get_caption(self, idx):\n",
    "        return self.dataframe.iloc[idx][\"caption\"]\n",
    "\n",
    "\n",
    "def display_image(index, dataset):\n",
    "    if index >= len(dataset):\n",
    "        raise ValueError(f\"Index is out of bounds for the dataset\")\n",
    "    image, _ = dataset[index]\n",
    "    image = image.permute(1, 2, 0).detach().numpy()\n",
    "    # image = (image * 255).astype('uint8')\n",
    "    height, width, _ = image.shape\n",
    "    plt.xlim(0, width)\n",
    "    plt.ylim(height, 0)\n",
    "    plt.imshow(image)\n",
    "    plt.title(dataset.get_caption(index))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "ex_dataset = Flickr30kImages(dataframe=train_df, transform=transform)\n",
    "dataloader = DataLoader(ex_dataset, batch_size=1, shuffle=True)\n",
    "display_image(2, ex_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define compare_transforms for later evaluation with blurred pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_transforms(transformations, index_image):\n",
    "    if not all(isinstance(transf, Dataset) for transf in transformations):\n",
    "        raise TypeError(\n",
    "            \"All elements in the `transformations` list need to be of type Dataset\"\n",
    "        )\n",
    "\n",
    "    num_tr = len(transformations)\n",
    "    fig, axes = plt.subplots(1, num_tr, figsize=(num_tr * 4, 4))\n",
    "\n",
    "    if num_tr == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    image, _ = transformations[0][index_image]\n",
    "\n",
    "    for counter, (axis, transf) in enumerate(zip(axes, transformations)):\n",
    "        image, _ = transf[index_image]\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = image.permute(1, 2, 0).detach().numpy()\n",
    "            if image.shape[2] == 3:\n",
    "                if image.max() <= 1.0:\n",
    "                    image = (image * 255).astype(\"uint8\")\n",
    "        axis.imshow(image)\n",
    "        axis.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(transformations[0].get_caption(index_image))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN model\n",
    "### VGG + GRU architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a first architecture for image captioning by combining the pre-trained VGG16 model and the GRU unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "\n",
    "\n",
    "class NN_conv_gru(nn.Module):\n",
    "    \"\"\"\n",
    "    A neural network model for image captioning that combines a pre-trained VGG16 convolutional neural network with a Gated Recurrent Unit (GRU). \n",
    "\n",
    "    This model is designed to extract features from input images using VGG16, which is initialized with weights pre-trained on the ImageNet dataset. \n",
    "    The model then processes these features through a GRU cell to generate sequences of predicted word distributions.\n",
    "\n",
    "    Attributes:\n",
    "        input_size (int): The size of the input features for the GRU.\n",
    "        hidden_size (int): The size of the hidden state in the GRU.\n",
    "        output_size (int): The size of the output layer, corresponding to the vocabulary size.\n",
    "        target_len (int): The length of the output sequence to generate.\n",
    "\n",
    "    Methods:\n",
    "        forward_cnn(input_batch): Extracts image features using the VGG16 model.\n",
    "        forward_gru(x, h): Processes the input through the GRU and generates output word scores.\n",
    "        forward(input): Combines feature extraction and sequence generation, returning a tensor of predicted word distributions based on the input images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, target_len):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.target_len = target_len\n",
    "\n",
    "        self.vgg_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)\n",
    "        feature_dim = self.vgg_model.classifier[0].in_features\n",
    "        self.vgg_model.classifier = nn.Identity()\n",
    "\n",
    "        for param in self.vgg_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.fc_between1 = nn.Linear(feature_dim, feature_dim // 4)  #to adapt to gru\n",
    "        self.fc_between2 = nn.Linear(feature_dim // 4, hidden_size)  \n",
    "\n",
    "        self.GRU = nn.GRUCell(input_size, hidden_size, bias=True, dtype=torch.float32)\n",
    "        self.h_to_words = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward_cnn(self, input_batch):\n",
    "        x = self.vgg_model(input_batch)\n",
    "        return x.squeeze()\n",
    "\n",
    "    def forward_gru(self, x, h):\n",
    "        h = self.GRU(x, h)\n",
    "        y = self.h_to_words(h)\n",
    "        return y, h\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.forward_cnn(input)\n",
    "\n",
    "        h = self.fc_between1(x)\n",
    "        h = self.fc_between2(h)\n",
    "        num_batches = input.shape[0]\n",
    "\n",
    "        y = torch.zeros((num_batches, self.target_len + 1, self.output_size))\n",
    "        y[:, 0, :] = our_vocab.one_hot_encode(\"<SOS>\")\n",
    "        y = y.to(x.device)\n",
    "        #feed the indexed caption as it grows until target_len\n",
    "\n",
    "        for t in range(self.target_len):\n",
    "            temp_y = y[:, t, :].clone()\n",
    "            temp_output, h = self.forward_gru(temp_y, h)\n",
    "            y = y.clone()\n",
    "            y[:, t + 1, :] = temp_output\n",
    "        y = y[:, 1:, :]  #remove the SOS token\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained the model on the train_dataset (approximately 110 000 pairs image-caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every\n",
    "):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model, train_loss = train_epoch(model, optimizer, loss_fn, train_loader, val_loader, device, print_every)\n",
    "        val_loss = validate(model, loss_fn, val_loader, device)\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{num_epochs}: \"\n",
    "            f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "            f\"Val. loss: {val_loss:.3f}, \"\n",
    "        )\n",
    "        train_losses.extend(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "\n",
    "def train_epoch(\n",
    "    model, optimizer, loss_fn, train_loader, val_loader, device, print_every\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    train_loss_batches = []\n",
    "    num_batches = len(train_loader)\n",
    "    for batch_index, (x, y) in enumerate(train_loader, 1):\n",
    "        inputs, targets = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "        logits = model.forward(inputs)\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        targets = targets.reshape(-1)\n",
    "\n",
    "        logits = logits.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        loss = loss_fn(logits, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        if batch_index % 25 == 0 or batch_index == 1:\n",
    "            print(f\"Batch {batch_index} at time {dt.now()}, loss: {loss.item()}\")\n",
    "\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(\n",
    "                f\"\\tBatch {batch_index}/{num_batches}: \"\n",
    "                f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                f\"\\tVal. loss: {val_loss:.3f}, \"\n",
    "            )\n",
    "\n",
    "    return model, train_loss_batches\n",
    "\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):\n",
    "    val_loss_cum = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(val_loader, 1):\n",
    "            inputs, targets = x.to(device), y.to(device)\n",
    "            logits = model.forward(inputs)\n",
    "            logits = logits.reshape(-1, logits.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "            batch_loss = loss_fn(logits, targets)\n",
    "            val_loss_cum += batch_loss.item()\n",
    "    return val_loss_cum / len(val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have saved the trained model in file ' ..... ', to avoid re-training it every time. \n",
    "\n",
    "The weights used in the loss function are the normalized wieghts extracted from the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = vocab_size \n",
    "hidden_size = 512\n",
    "output_size = input_size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = NN_conv_gru(input_size, hidden_size, output_size, target_len)\n",
    "learning_rate = 0.0000005\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "num_epochs = 1\n",
    "batch_size = 128\n",
    "\n",
    "#To feed the images to VGG\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "transform = transforms.Compose(\n",
    "    [Resize((224, 224)), ToTensor(), Normalize(mean, std, inplace=True)]\n",
    ")\n",
    "\n",
    "train_dataset = Flickr30kImages(dataframe=train_df, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "val_dataset = Flickr30kImages(dataframe=val_df, transform=transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=4)\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}, Validation batches: {len(val_loader)}\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "checkpoint = torch.load(\"VGG-GRU-model_v7.ckpt\", map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "del checkpoint\n",
    "\n",
    "if False:\n",
    "    model, train_losses, val_losses = training_loop(\n",
    "        model,\n",
    "        optimizer,\n",
    "        loss_fn,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs,\n",
    "        print_every=len(train_loader) // 3,\n",
    "    )\n",
    "\n",
    "    time = dt.now().strftime(\"%Y-%m-%d__%H:%M:%S\")\n",
    "    torch.save(\n",
    "        {\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "        },\n",
    "        f\"./VGG-GRU-model_{time}.ckpt\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results' analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To qualitatively analyze the results, we need a function which converts the output of the model, that is a distribution of probabilities, into word indices, according to our Vocabulary class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_captions(logits, vocab):\n",
    "    \"\"\"\n",
    "    Converts output logits from the neural network into a human-readable caption string.\n",
    "\n",
    "    This function processes the logits and extracts the most likely word indices. \n",
    "    The predicted indices are then mapped to words using the provided vocabulary, stopping the conversion when the end-of-sequence token (\"<EOS>\") is encountered. \n",
    "\n",
    "    Parameters:\n",
    "        logits (torch.Tensor): A tensor containing predicted word scores, either in shape (batch_size, vocab_size) or (vocab_size).\n",
    "        vocab (Vocabulary): An object that contains a mapping from indices to words (idx2word).\n",
    "\n",
    "    Returns:\n",
    "        str: The generated caption as a space-separated string of words.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(logits.shape) == 2:\n",
    "        _, predicted_indices = logits.max(dim=-1)\n",
    "    else:\n",
    "        predicted_indices = logits\n",
    "    caption = []\n",
    "    for idx in predicted_indices:\n",
    "        word = vocab.idx2word[idx.item()]\n",
    "        if word == \"<EOS>\":\n",
    "            break\n",
    "        if word != \"<SOS>\":\n",
    "            caption.append(word)\n",
    "    caption_string = \" \".join(caption)\n",
    "    return caption_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30kImages_names(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.dataframe.iloc[idx][\"image_name\"]\n",
    "        caption = self.dataframe.iloc[idx][\"caption\"]\n",
    "        tensor_captioning_indices = caption_to_padded_indices(\n",
    "            our_vocab, caption, target_len\n",
    "        )\n",
    "\n",
    "        image_path = os.path.join(images_path, image_name)\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, image_name\n",
    "\n",
    "    def get_caption(self, idx):\n",
    "        return self.dataframe.iloc[idx][\"caption\"]\n",
    "\n",
    "    def get_image_name(self, idx):\n",
    "        return self.dataframe.iloc[idx][\"image_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    test_dataset = Flickr30kImages_names(dataframe=test_df, transform=transform)\n",
    "    test_loader = DataLoader(val_dataset, batch_size=32, num_workers=0)\n",
    "    print(f\"Starting testing loop with {len(test_loader)} batches\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "    test_predicted_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(test_loader):\n",
    "            print(f\"Batch {i+1}/{len(test_loader)}\")\n",
    "            logits = model.forward(x.to(device)).cpu()\n",
    "            for batch_idx in range(logits.size(0)):\n",
    "                generated_caption = logits_to_captions(\n",
    "                    logits[batch_idx, :, :], our_vocab\n",
    "                )\n",
    "                image_name = y[batch_idx]\n",
    "                test_predicted_dict[image_name] = generated_caption\n",
    "\n",
    "    torch.save(test_predicted_dict, \"test_predicted_dict.pkl\")\n",
    "else:\n",
    "    test_predicted_dict = torch.load(\"test_predicted_dict_GRU.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predicted_dict = img_name -> caption\n",
    "# filtered_ground_truth_dict = img_name -> [caption]\n",
    "\n",
    "if False:\n",
    "    test_ground_truth_dict = {}\n",
    "    for img_name in test_predicted_dict.keys():\n",
    "        captions_list = image_caption_df[image_capidx_df[\"image_name\"] == img_name][\n",
    "            \"caption\"\n",
    "        ].tolist()\n",
    "        test_ground_truth_dict[img_name] = []\n",
    "\n",
    "        for caption in captions_list:\n",
    "            capt = caption.split(\" \")\n",
    "            if not len(capt) <= target_len - 1:  # -1 because of <EOS>\n",
    "                capt = capt[:target_len]\n",
    "            test_ground_truth_dict[img_name].append(\" \".join(capt))\n",
    "\n",
    "    torch.save(test_ground_truth_dict, \"test_ground_truth_dict.pkl\")\n",
    "else:\n",
    "    test_ground_truth_dict = torch.load(\"test_ground_truth_dict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantitavely evaluate the model's performance, we use CIDEr, METEOR, ROUGE-1, ROUGE-2 and ROUGE-L scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocoevalcap.cider.cider import Cider\n",
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "cider_scorer = Cider()\n",
    "cider_score, _ = cider_scorer.compute_score(test_ground_truth_dict, test_predicted_dict)\n",
    "\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "predictions = [test_predicted_dict[img_name][0] for img_name in test_predicted_dict]\n",
    "references = [test_ground_truth_dict[img_name][0] for img_name in test_ground_truth_dict]\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "rouge_1_scores = []\n",
    "rouge_2_scores = []\n",
    "rouge_l_scores = []\n",
    "for image_name in test_predicted_dict.keys():\n",
    "    references = test_ground_truth_dict[image_name]\n",
    "    prediction = test_predicted_dict[image_name][0]\n",
    "    scores = scorer.score(references[0], prediction)\n",
    "    rouge_1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge_2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rouge_l_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "average_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "average_rouge_2 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "\n",
    "print(f\"CIDEr score for test dataset: {cider_score}\")\n",
    "print(f\"METEOR score: {meteor_score['meteor']}\")\n",
    "print(f\"Average ROUGE-1 Score: {average_rouge_1}\")\n",
    "print(f\"Average ROUGE-2 Score: {average_rouge_2}\")\n",
    "print(f\"Average ROUGE-L Score: {average_rouge_l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on blurred images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A further evaluation to test the robustness of the model to variations in input quality can be done by applying GaussianBlur to the images in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_blur = transforms.Compose(\n",
    "    [\n",
    "        GaussianBlur(kernel_size=31, sigma=3),\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean, std, inplace=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    blur_test_dataset = Flickr30kImages_names(dataframe=test_df, transform=None)\n",
    "    blur_test_loader = DataLoader(blur_test_dataset, batch_size=1, num_workers=2)\n",
    "\n",
    "    print(f\"Starting testing loop with {len(test_loader)} batches\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(device)\n",
    "    model.to(device)\n",
    "\n",
    "    blur_predicted_dict = {}\n",
    "    with torch.no_grad():\n",
    "        for i, (image, imagename) in enumerate(test_dataset):\n",
    "            if i%100==0:\n",
    "                print(f\"Batch {i+1}/{len(test_dataset)}\")\n",
    "\n",
    "            \n",
    "            #image = torchvision.transforms.functional.to_pil_image(x[0])\n",
    "            image = GaussianBlur(kernel_size=31, sigma=3)(image)\n",
    "            \n",
    "            pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "\n",
    "            generated_ids = model.generate(pixel_values=pixel_values.clone().to(device), max_length=80)\n",
    "            generated_caption = processor.batch_decode(\n",
    "                generated_ids, skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "            blur_predicted_dict[imagename] = generated_caption\n",
    "            #print(generated_caption)\n",
    "                \n",
    "    torch.save(blur_predicted_dict, \"blur_predicted_dict_GIT_blur.pkl\")\n",
    "else:\n",
    "    pass\n",
    "    #blur_predicted_dict = torch.load(\"blur_predicted_dict_GRU.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cider_scorer = Cider()\n",
    "cider_score, _ = cider_scorer.compute_score(test_ground_truth_dict, blur_predicted_dict)\n",
    "\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "predictions = [blur_predicted_dict[img_name][0] for img_name in blur_predicted_dict]\n",
    "references = [test_ground_truth_dict[img_name][0] for img_name in test_ground_truth_dict]\n",
    "meteor_score = meteor.compute(predictions=predictions, references=references)\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "rouge_1_scores = []\n",
    "rouge_2_scores = []\n",
    "rouge_l_scores = []\n",
    "for image_name in blur_predicted_dict.keys():\n",
    "    references = test_ground_truth_dict[image_name]\n",
    "    prediction = blur_predicted_dict[image_name][0]\n",
    "    scores = scorer.score(references[0], prediction)\n",
    "    rouge_1_scores.append(scores[\"rouge1\"].fmeasure)\n",
    "    rouge_2_scores.append(scores[\"rouge2\"].fmeasure)\n",
    "    rouge_l_scores.append(scores[\"rougeL\"].fmeasure)\n",
    "average_rouge_1 = sum(rouge_1_scores) / len(rouge_1_scores)\n",
    "average_rouge_2 = sum(rouge_2_scores) / len(rouge_2_scores)\n",
    "average_rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "\n",
    "print(f\"CIDEr score for blurred test dataset: {cider_score}\")\n",
    "print(f\"METEOR score: {meteor_score['meteor']}\")\n",
    "print(f\"Average ROUGE-1 Score: {average_rouge_1}\")\n",
    "print(f\"Average ROUGE-2 Score: {average_rouge_2}\")\n",
    "print(f\"Average ROUGE-L Score: {average_rouge_l}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE to revise!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Flickr30kImages(dataframe=test_df, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=3, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(val_loader):  \n",
    "        logits = model.forward(x.to(device))       \n",
    "        for i in range(logits.size(0)):\n",
    "            image_name = test_loader.dataset.get_image_name(batch_idx * test_loader.batch_size + i)  \n",
    "            print( logits_to_captions(logits[i, :, :], our_vocab),  \" ||| \",  image_name,  logits_to_captions(y[batch_idx, :], our_vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_predicted_dict = img_name -> caption\n",
    "# filtered_ground_truth_dict = img_name -> [caption]\n",
    "\n",
    "if False:\n",
    "    test_ground_truth_dict = {}\n",
    "    for img_name in test_predicted_dict.keys():\n",
    "        captions_list = image_caption_df[image_capidx_df[\"image_name\"] == img_name][\n",
    "            \"caption\"\n",
    "        ].tolist()\n",
    "        test_ground_truth_dict[img_name] = []\n",
    "\n",
    "        for caption in captions_list:\n",
    "            capt = caption.split(\" \")\n",
    "            if not len(capt) <= target_len - 1:  # -1 because of <EOS>\n",
    "                capt = capt[:target_len]\n",
    "            test_ground_truth_dict[img_name].append(\" \".join(capt))\n",
    "\n",
    "    torch.save(test_ground_truth_dict, \"test_ground_truth_dict.pkl\")\n",
    "else:\n",
    "    test_ground_truth_dict = torch.load(\"test_ground_truth_dict.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_test_dataset = Flickr30kImages(dataframe=test_df, transform=transform_blur)\n",
    "blur_test_loader = DataLoader(blur_test_dataset, batch_size=3, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (x, _) in enumerate(val_loader):  \n",
    "        logits = model.forward(x.to(device))       \n",
    "        for i in range(logits.size(0)):\n",
    "            image_name = blur_test_loader.dataset.get_image_name(batch_idx * blur_test_loader.batch_size + i)  \n",
    "            print( logits_to_captions(logits[i, :, :], our_vocab),  \" ||| \",  image_name,  logits_to_captions(y[batch_idx, :], our_vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
